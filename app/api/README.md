# Digital Me API

The `app/api` module serves as the backend intelligence for Digital Me. It exposes REST endpoints to process chat messages, retrieve context from the vector store, and generate responses using Ollama.

## Endpoints

### `GET /api`

**Description:** Health check endpoint.
**Response:** `200 OK` - "Digital-Me is running"

### `POST /api`

**Description:** Main chat completion endpoint.
**Request Body:**

```json
{
  "messages": [
    { "role": "user", "content": "Who are you?" },
    { "role": "assistant", "content": "I am..." }
    // ... history
  ]
}
```

**Response:** Stream of text chunks (Content-Type: `text/plain` or `application/octet-stream` depending on client handling).

## How it Works

1.  **Request Handling**: Receives the chat history array from the client (CLI/Web).
2.  **Context Retrieval (RAG)**:
    - Extracts the last user message.
    - Queries the local `VectorStore` for semantically similar documents (resume chunks, GitHub activity, static JSON data).
    - If relevant documents are found, they are prepended to the system prompt as "Relevant Context".
3.  **Prompt Construction**:
    - Combines the System Prompt (defining the persona) + Retrieved Context + Chat History.
4.  **LLM Inference**:
    - Sends the constructed prompt to the local Ollama instance (`llama3` model).
    - Streams the response token-by-token back to the client.

## Integration

### Ollama

The API assumes Ollama is running locally on default port `11434`. It uses the `ollama` npm package to interface with the model.

### Vector Store

The API shares the `memory/vector_store` with the ingestion pipeline. It performs similarity search using cosine similarity on embeddings generated by `compute-cosine-similarity` and simple keyword matching (if configured).

## Security Note

This API is currently designed for local use. It does not implement authentication. If deploying, ensure to add API key verification or run behind a secure proxy.
